---
##############################################################################
# Copyright (c) 2025, Lawrence Livermore National Security, LLC and RADIUSS
# project contributors. See the COPYRIGHT file for details.
#
# SPDX-License-Identifier: (MIT)
##############################################################################

# GitLab CI Component: Performance Pipeline
#
# This component provides templates for running performance measurements
# across LLNL supercomputers, processing results, and reporting to GitHub.
#
# Usage:
#   include:
#     - component: $CI_SERVER_FQDN/radiuss/radiuss-shared-ci/performance-pipeline@v2025.12.0
#       inputs:
#         job_cmd: "./scripts/run-benchmarks.sh"
#         perf_processing_cmd: "./scripts/convert-to-gh-benchmark.py"
#         <machine>_perf_alloc: <machine job allocation args>

spec:
  inputs:
    job_cmd:
      type: string
      description: "Command to execute for performance measurements"

    dane_perf_alloc:
      type: string
      description: "SLURM srun arguments for job allocation on Dane"
      default: ""

    matrix_perf_alloc:
      type: string
      description: "SLURM srun arguments for job allocation on Matrix"
      default: ""

    corona_perf_alloc:
      type: string
      description: "Flux alloc arguments for job allocation on Corona"
      default: ""

    tioga_perf_alloc:
      type: string
      description: "Flux alloc arguments for job allocation on Tioga"
      default: ""

    tuolumne_perf_alloc:
      type: string
      description: "Flux alloc arguments for job allocation on Tuolumne"
      default: ""

    lassen_perf_alloc:
      type: string
      description: "Lalloc arguments for job allocation on Lassen"
      default: ""

    perf_processing_cmd:
      type: string
      description: "Command to process performance results"
      default: ""

    perf_artifact_dir:
      type: string
      description: "Directory for performance artifacts"
      default: "performance-results"

    perf_results_file:
      type: string
      description: "Name of raw results file"
      default: "benchmark_results.json"

    perf_processed_file:
      type: string
      description: "Name of processed results file"
      default: "processed_results.json"

    github_token:
      type: string
      description: "GitHub token for reporting results"
      default: ""

    github_project_name:
      type: string
      description: "GitHub project name"
      default: ""

    github_project_org:
      type: string
      description: "GitHub organization name"
      default: ""

---
# Sets ID tokens for every job using `default:`
include:
  - project: 'lc-templates/id_tokens'
    file: 'id_tokens.yml'

variables:
  PERF_ARTIFACT_DIR: "$[[ inputs.perf_artifact_dir ]]"
  PERF_RESULTS_FILE: "$[[ inputs.perf_results_file ]]"
  PERF_PROCESSED_FILE: "$[[ inputs.perf_processed_file ]]"
  JOB_CMD: "$[[ inputs.job_cmd | expand_vars ]]"
  PERF_PROCESSING_CMD: "$[[ inputs.perf_processing_cmd ]]"
  GITHUB_TOKEN: "$[[ inputs.github_token ]]"
  GITHUB_PROJECT_NAME: "$[[ inputs.github_project_name ]]"
  GITHUB_PROJECT_ORG: "$[[ inputs.github_project_org ]]"

stages:
  - perf-runs
  - perf-processing
  - perf-reporting

##############################################################################
# PERFORMANCE JOB TEMPLATES (machine-specific)

.perf_job:
  artifacts:
    paths:
      - $PERF_ARTIFACT_DIR/

# Custom job template specific to performance pipeline.
# Override this to create project-specific setup.
.custom_perf:
  variables:
    TEMPLATE_CANNOT_BE_EMPTY: "true"

.on_dane:
  tags:
    - shell
    - dane
  rules:
    - if: '$ON_DANE == "OFF"'
      when: never
    - if: '"$[[ inputs.dane_perf_alloc ]]" == ""'
      when: never
    - when: on_success

.on_matrix:
  tags:
    - shell
    - matrix
  rules:
    - if: '$ON_MATRIX == "OFF"'
      when: never
    - if: '"$[[ inputs.matrix_perf_alloc ]]" == ""'
      when: never
    - when: on_success

.on_corona:
  tags:
    - shell
    - corona
  rules:
    - if: '$ON_CORONA == "OFF"'
      when: never
    - if: '"$[[ inputs.corona_perf_alloc ]]" == ""'
      when: never
    - when: on_success

.on_tioga:
  tags:
    - shell
    - tioga
  rules:
    - if: '$ON_TIOGA == "OFF"'
      when: never
    - if: '"$[[ inputs.tioga_perf_alloc ]]" == ""'
      when: never
    - when: on_success

.on_tuolumne:
  tags:
    - shell
    - tuolumne
  rules:
    - if: '$ON_TUOLUMNE == "OFF"'
      when: never
    - if: '"$[[ inputs.tuolumne_perf_alloc ]]" == ""'
      when: never
    - when: on_success

.on_lassen:
  tags:
    - shell
    - lassen
  rules:
    - if: '$ON_LASSEN == "OFF"'
      when: never
    - if: '"$[[ inputs.lassen_perf_alloc ]]" == ""'
      when: never
    - when: on_success

.perf_on_dane:
  extends: [.perf_job, .on_dane, .custom_perf]
  stage: perf-runs
  script:
    - export PERF_ARTIFACT_DIR=${PERF_ARTIFACT_DIR}/${CI_JOB_NAME}
    - mkdir -p ${PERF_ARTIFACT_DIR}
    - srun $[[ inputs.dane_perf_alloc ]] ${JOB_CMD}

.perf_on_matrix:
  extends: [.perf_job, .on_matrix, .custom_perf]
  stage: perf-runs
  script:
    - export PERF_ARTIFACT_DIR=${PERF_ARTIFACT_DIR}/${CI_JOB_NAME}
    - mkdir -p ${PERF_ARTIFACT_DIR}
    - srun $[[ inputs.matrix_perf_alloc ]] ${JOB_CMD}

.perf_on_corona:
  extends: [.perf_job, .on_corona, .custom_perf]
  stage: perf-runs
  script:
    - export PERF_ARTIFACT_DIR=${PERF_ARTIFACT_DIR}/${CI_JOB_NAME}
    - mkdir -p ${PERF_ARTIFACT_DIR}
    - srun $[[ inputs.corona_perf_alloc ]] ${JOB_CMD}

.perf_on_tioga:
  extends: [.perf_job, .on_tioga, .custom_perf]
  stage: perf-runs
  script:
    - export PERF_ARTIFACT_DIR=${PERF_ARTIFACT_DIR}/${CI_JOB_NAME}
    - mkdir -p ${PERF_ARTIFACT_DIR}
    - flux run $[[ inputs.tioga_perf_alloc ]] ${JOB_CMD}

.perf_on_tuolumne:
  extends: [.perf_job, .on_tuolumne, .custom_perf]
  stage: perf-runs
  script:
    - export PERF_ARTIFACT_DIR=${PERF_ARTIFACT_DIR}/${CI_JOB_NAME}
    - mkdir -p ${PERF_ARTIFACT_DIR}
    - flux run $[[ inputs.tuolumne_perf_alloc ]] ${JOB_CMD}

.perf_on_lassen:
  extends: [.perf_job, .on_lassen, .custom_perf]
  stage: perf-runs
  script:
    - export PERF_ARTIFACT_DIR=${PERF_ARTIFACT_DIR}/${CI_JOB_NAME}
    - mkdir -p ${PERF_ARTIFACT_DIR}
    - lalloc $[[ inputs.lassen_perf_alloc ]] ${JOB_CMD}

##############################################################################
# PROCESSING AND REPORTING TEMPLATES

# Minimal templates for performance processing and reporting jobs
.results_processing:
  stage: perf-processing
  tags:
    - shell
    - oslic
  artifacts:
    paths:
      - $PERF_ARTIFACT_DIR/
  variables:
    GIT_SUBMODULE_STRATEGY: none

.results_reporting:
  stage: perf-reporting
  tags:
    - shell
    - oslic
  variables:
    GIT_STRATEGY: none
    GIT_SUBMODULE_STRATEGY: none

# Convert to Github Benchmark templates:
# Projects provide their own ${PERF_PROCESSING_CMD} script to convert
# the data to github benchmark format.
.convert_to_gh_benchmark:
  extends: .results_processing
  script:
    - |
      for dir in ${PERF_ARTIFACT_DIR}/*/; do
        if [ -n "${PERF_PROCESSING_CMD}" ]; then
          if [ -d "$dir" ]; then
            echo "Processing results in ${dir}"
            cd ${dir}
            ${PERF_PROCESSING_CMD} "${PERF_RESULTS_FILE}" "${PERF_PROCESSED_FILE}"
            cd -
          fi
        else
          echo "[Warning] Unless processing is not required, PERF_PROCESSING_CMD should be defined."
        fi
      done

.caliper_to_gh_benchmark:
  extends: .convert_to_gh_benchmark
  before_script:
    - |
      python3 -m venv caliper-env
      source caliper-env/bin/activate
      pip install caliper-reader

# Report performance results to GitHub
.report_to_gh_benchmark:
  extends: .results_reporting
  script:
    - |
      for file in ${PERF_ARTIFACT_DIR}/*/${PERF_PROCESSED_FILE}; do
        if [ -f "$file" ]; then
          # Check if required variables are set
          if [ -z "$GITHUB_TOKEN" ] || [ -z "$GITHUB_PROJECT_ORG" ] || [ -z "$GITHUB_PROJECT_NAME" ]; then
            echo "Error: Required GitHub variables not set"
            exit 1
          fi

          # Send job name as the benchmark name
          BENCHMARK_NAME=$(basename "$(dirname "$file")")
          echo "Sending benchmark results to GitHub for ${BENCHMARK_NAME}..."
          BENCHMARK_DATA=$(base64 -w 0 "$file")
          RESPONSE_BODY=$(mktemp)
          HTTP_CODE=$(curl --retry 3 --retry-delay 5 --retry-connrefused --max-time 30 -X POST \
               --url "https://api.github.com/repos/${GITHUB_PROJECT_ORG}/${GITHUB_PROJECT_NAME}/actions/workflows/benchmark.yml/dispatches" \
               --header "Authorization: token $GITHUB_TOKEN" \
               --header "Accept: application/vnd.github.v3+json" \
               --header "Content-Type: application/json" \
               --data "{ \"ref\": \"${CI_COMMIT_REF_NAME}\", \"inputs\": { \"benchmark_name\": \"${BENCHMARK_NAME}\", \"benchmark_data\": \"${BENCHMARK_DATA}\" } }" \
               --output "$RESPONSE_BODY" \
               --write-out "%{http_code}" \
               --silent \
               --show-error
               )

          echo "GitHub API response code: $HTTP_CODE"
          if [ "$HTTP_CODE" -eq 204 ]; then
            echo "Successfully triggered GitHub workflow for ${BENCHMARK_NAME}"
          else
            echo "Failed to trigger GitHub workflow for ${BENCHMARK_NAME}. HTTP status: $HTTP_CODE"
            echo "Response body:"
            cat "$RESPONSE_BODY"
            rm -f "$RESPONSE_BODY"
            exit 1
          fi
          rm -f "$RESPONSE_BODY"
        else
          echo "$file not found, skipping GitHub API integration."
        fi
      done
